{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc940c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model: processed_dataset\\isl_gesture_model.h5\n",
      "Webcam opened. Press 'q' to quit.\n",
      "✅ Real-time demo stopped.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Part F: Real-time Webcam Demo (.h5 LSTM) with Landmarks\n",
    "# -------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# --- Parameters\n",
    "SEQUENCE_LENGTH = 32\n",
    "FEATURE_SIZE = 138\n",
    "MODEL_SAVE_PATH = os.path.join(\"processed_dataset\", \"isl_gesture_model.h5\")\n",
    "PROCESSED_DIR = \"processed_dataset\"\n",
    "\n",
    "# --- Load label map\n",
    "with open(os.path.join(PROCESSED_DIR, \"label_map.pkl\"), \"rb\") as f:\n",
    "    label_to_idx = pickle.load(f)\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "\n",
    "# --- Load trained LSTM model\n",
    "model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
    "print(f\"✅ Loaded model: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# --- Initialize MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "holistic = mp_holistic.Holistic(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    refine_face_landmarks=False\n",
    ")\n",
    "\n",
    "# --- Feature extraction function (138 features)\n",
    "def extract_features(frame, holistic):\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image_rgb)\n",
    "    vec = []\n",
    "\n",
    "    # Pose: shoulders (11=left, 12=right)\n",
    "    if results.pose_landmarks:\n",
    "        for idx in [11, 12]:\n",
    "            lm = results.pose_landmarks.landmark[idx]\n",
    "            vec.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        vec.extend([0.0]*3*2)\n",
    "\n",
    "    # Hands: 21 landmarks each\n",
    "    for hand in [results.left_hand_landmarks, results.right_hand_landmarks]:\n",
    "        if hand:\n",
    "            for lm in hand.landmark:\n",
    "                vec.extend([lm.x, lm.y, lm.z])\n",
    "        else:\n",
    "            vec.extend([0.0]*3*21)\n",
    "\n",
    "    # Palms: index 0 each hand\n",
    "    if results.left_hand_landmarks:\n",
    "        vec.extend([results.left_hand_landmarks.landmark[0].x,\n",
    "                    results.left_hand_landmarks.landmark[0].y,\n",
    "                    results.left_hand_landmarks.landmark[0].z])\n",
    "    else:\n",
    "        vec.extend([0.0]*3)\n",
    "    if results.right_hand_landmarks:\n",
    "        vec.extend([results.right_hand_landmarks.landmark[0].x,\n",
    "                    results.right_hand_landmarks.landmark[0].y,\n",
    "                    results.right_hand_landmarks.landmark[0].z])\n",
    "    else:\n",
    "        vec.extend([0.0]*3)\n",
    "\n",
    "    return np.array(vec, dtype=float) if len(vec) == FEATURE_SIZE else np.zeros(FEATURE_SIZE, dtype=float), results\n",
    "\n",
    "# --- Sliding window buffer\n",
    "buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "last_pred_text = None\n",
    "last_time = time.time()\n",
    "\n",
    "# --- Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot open webcam\")\n",
    "\n",
    "print(\"Webcam opened. Press 'q' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to read frame.\")\n",
    "            break\n",
    "\n",
    "        # Extract features and get landmarks\n",
    "        lm, results = extract_features(frame, holistic)\n",
    "        buffer.append(lm)\n",
    "\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # --- Draw pose landmarks (shoulders) ---\n",
    "        if results.pose_landmarks:\n",
    "            left_shoulder = results.pose_landmarks.landmark[11]\n",
    "            right_shoulder = results.pose_landmarks.landmark[12]\n",
    "            cv2.circle(frame, (int(left_shoulder.x*w), int(left_shoulder.y*h)), 8, (0,255,0), -1)\n",
    "            cv2.circle(frame, (int(right_shoulder.x*w), int(right_shoulder.y*h)), 8, (0,255,0), -1)\n",
    "\n",
    "        # --- Draw hand landmarks + palms ---\n",
    "        for hand_landmarks, color in zip([results.left_hand_landmarks, results.right_hand_landmarks],\n",
    "                                        [(0,0,255),(255,0,0)]):\n",
    "            if hand_landmarks:\n",
    "                # Draw all hand landmarks\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                          landmark_drawing_spec=mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=3),\n",
    "                                          connection_drawing_spec=mp_drawing.DrawingSpec(color=color, thickness=2))\n",
    "                # Highlight palm (index 0)\n",
    "                lm0 = hand_landmarks.landmark[0]\n",
    "                cv2.circle(frame, (int(lm0.x*w), int(lm0.y*h)), 12, color, -1)\n",
    "\n",
    "        # Predict gesture if buffer full\n",
    "        if len(buffer) == SEQUENCE_LENGTH:\n",
    "            seq_input = np.array(buffer, dtype=np.float32).reshape(1, SEQUENCE_LENGTH, FEATURE_SIZE)\n",
    "            pred = model.predict(seq_input, verbose=0)[0]\n",
    "            idx = int(np.argmax(pred))\n",
    "            conf = float(pred[idx])\n",
    "            label = idx_to_label.get(idx, \"Unknown\")\n",
    "            last_pred_text = f\"{label} ({conf*100:.1f}%)\"\n",
    "            last_time = time.time()\n",
    "\n",
    "        # Display prediction\n",
    "        if last_pred_text:\n",
    "            cv2.rectangle(frame, (10, 10), (450, 60), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, last_pred_text, (20, 45),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show webcam frame\n",
    "        cv2.imshow(\"ISL Gesture Real-Time Demo\", frame)\n",
    "\n",
    "        # Quit on 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    holistic.close()\n",
    "    print(\"✅ Real-time demo stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b86af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TFLite model: processed_dataset\\isl_gesture_model.tflite\n",
      "Webcam opened. Press 'q' to quit.\n",
      "Real-time demo stopped.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Real-time Webcam Demo using .tflite LSTM model (with face landmarks to hide face)\n",
    "# -------------------------------\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# --- Parameters ---\n",
    "SEQUENCE_LENGTH = 32\n",
    "FEATURE_SIZE = 138\n",
    "MODEL_PATH = os.path.join(\"processed_dataset\", \"isl_gesture_model.tflite\")\n",
    "PROCESSED_DIR = \"processed_dataset\"\n",
    "\n",
    "# --- Load label map ---\n",
    "with open(os.path.join(PROCESSED_DIR, \"label_map.pkl\"), \"rb\") as f:\n",
    "    label_to_idx = pickle.load(f)\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "\n",
    "# --- Load TensorFlow Lite model ---\n",
    "interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(f\"Loaded TFLite model: {MODEL_PATH}\")\n",
    "\n",
    "# --- Initialize MediaPipe Holistic ---\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "holistic = mp_holistic.Holistic(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    refine_face_landmarks=False\n",
    ")\n",
    "\n",
    "# --- Feature extraction function (138 features) ---\n",
    "def extract_features(frame, holistic):\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image_rgb)\n",
    "    vec = []\n",
    "\n",
    "    # Pose: shoulders (11=left, 12=right)\n",
    "    if results.pose_landmarks:\n",
    "        for idx in [11, 12]:\n",
    "            lm = results.pose_landmarks.landmark[idx]\n",
    "            vec.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        vec.extend([0.0]*3*2)\n",
    "\n",
    "    # Hands: 21 landmarks each\n",
    "    for hand in [results.left_hand_landmarks, results.right_hand_landmarks]:\n",
    "        if hand:\n",
    "            for lm in hand.landmark:\n",
    "                vec.extend([lm.x, lm.y, lm.z])\n",
    "        else:\n",
    "            vec.extend([0.0]*3*21)\n",
    "\n",
    "    # Palms: index 0 each hand\n",
    "    if results.left_hand_landmarks:\n",
    "        lm = results.left_hand_landmarks.landmark[0]\n",
    "        vec.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        vec.extend([0.0]*3)\n",
    "    if results.right_hand_landmarks:\n",
    "        lm = results.right_hand_landmarks.landmark[0]\n",
    "        vec.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        vec.extend([0.0]*3)\n",
    "\n",
    "    return (\n",
    "        np.array(vec, dtype=float)\n",
    "        if len(vec) == FEATURE_SIZE\n",
    "        else np.zeros(FEATURE_SIZE, dtype=float),\n",
    "        results\n",
    "    )\n",
    "\n",
    "# --- Sliding window buffer ---\n",
    "buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "last_pred_text = None\n",
    "last_time = time.time()\n",
    "\n",
    "# --- Open webcam ---\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot open webcam\")\n",
    "\n",
    "print(\"Webcam opened. Press 'q' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to read frame.\")\n",
    "            break\n",
    "\n",
    "        # Extract features and landmarks\n",
    "        lm, results = extract_features(frame, holistic)\n",
    "        buffer.append(lm)\n",
    "\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # --- Draw pose landmarks (shoulders) ---\n",
    "        if results.pose_landmarks:\n",
    "            left_shoulder = results.pose_landmarks.landmark[11]\n",
    "            right_shoulder = results.pose_landmarks.landmark[12]\n",
    "            cv2.circle(frame, (int(left_shoulder.x*w), int(left_shoulder.y*h)), 8, (0,255,0), -1)\n",
    "            cv2.circle(frame, (int(right_shoulder.x*w), int(right_shoulder.y*h)), 8, (0,255,0), -1)\n",
    "\n",
    "        # --- Draw face landmarks (to hide face) ---\n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1)\n",
    "            )\n",
    "\n",
    "        # --- Draw hand landmarks + palms ---\n",
    "        for hand_landmarks, color in zip(\n",
    "            [results.left_hand_landmarks, results.right_hand_landmarks],\n",
    "            [(0,0,255), (255,0,0)]\n",
    "        ):\n",
    "            if hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    mp_holistic.HAND_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=3),\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(color=color, thickness=2)\n",
    "                )\n",
    "                lm0 = hand_landmarks.landmark[0]\n",
    "                cv2.circle(frame, (int(lm0.x*w), int(lm0.y*h)), 12, color, -1)\n",
    "\n",
    "        # --- Run inference when buffer full ---\n",
    "        if len(buffer) == SEQUENCE_LENGTH:\n",
    "            seq_input = np.array(buffer, dtype=np.float32).reshape(1, SEQUENCE_LENGTH, FEATURE_SIZE)\n",
    "            interpreter.set_tensor(input_details[0]['index'], seq_input)\n",
    "            interpreter.invoke()\n",
    "            pred = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "            idx = int(np.argmax(pred))\n",
    "            conf = float(pred[idx])\n",
    "            label = idx_to_label.get(idx, \"Unknown\")\n",
    "            last_pred_text = f\"{label} ({conf*100:.1f}%)\"\n",
    "            last_time = time.time()\n",
    "\n",
    "        # --- Display prediction text ---\n",
    "        if last_pred_text:\n",
    "            cv2.rectangle(frame, (10, 10), (450, 60), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, last_pred_text, (20, 45),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"ISL Gesture Real-Time Demo (.tflite)\", frame)\n",
    "\n",
    "        # Quit on 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    holistic.close()\n",
    "    print(\"Real-time demo stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320235f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95719127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
